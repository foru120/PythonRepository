{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Training in TensorFlow\n",
    "\n",
    "This is a short introduction to multi-GPU training in TensorFlow. We will cover the following topics:\n",
    "\n",
    "* training your model with multiple GPUs on your local machine by using in-graph replication, and\n",
    "* training your model using distributed TensorFlow.\n",
    "\n",
    "In the following, I will assume that you have multiple *devices* (multiple GPUs, CPUs) that you want to make use of when training your model. They could either be installed in your local machine or in multiple machines across a network cluster.\n",
    "\n",
    "Before we get started on this, here are some general points about training models in parallel:\n",
    "There are, generally speaking, at least two aspects two consider when parallelizing a model.\n",
    "\n",
    "1. First, how should the model be parallelized? One possibility would be to divide a model in various *horziontal slices* and parallelize those, i.e. the first device handles the first few steps of the computation, the second device next steps etc. This means that each device will have more memory available for its computation, allowing you to increase the minibatch size. Alternatively, *vertical slices* of the model could be distributed across devices, meaning that each device goes through all steps of the model. This means that you effectively multiply your minibatch size with the number of your devices.\n",
    "The general consensus seems to be that the second method, called *replicated training*, is superior to the first, if only for its simplicity: Its very easy to add more and more devices without spending any thought on how the model needs to be chopped up to accomodate these additional devices. The downside to this approach is that all the devices need to share weights for the model, which implies that some mechanism for synchronizing these must be in place.\n",
    "\n",
    "2. Second, assuming we settled for the second method, how does the synchronization of variables work across the models? This depends on your setup: In a networked setting, you will probably use multiple instances of TensorFlow (*between graph replication*), with one (or multiple) of these designated as *parameter servers*. If you have multiple devices on your local machine, it might be a better idea to explicitly do the replication in a single TensorFlow instance (in-graph replication) and have a designated *controller device* (such as the CPU) that will keep track of the variables in the graph.\n",
    "During training, each of the replicas will perform a forward pass, followed by a backward pass, and send its gradient updates to the controller/parameter server, which applies the update and sends an update back to the device. This can either happen *synchronously* (the devices wait for each other to finish and always use the same weights) or *asynchronously* (devices operate without locking to each other, therefore each device may have slightly different parameters). Asynchronous training can reduce training times but is known to lead to slightly worse predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Expect From Multi-GPU Training\n",
    "\n",
    "Multi-GPU training is not the solution to all of your problems. Let's discuss some misconceptions:\n",
    "\n",
    "* The training speed does **not** scale linearly with the number of GPUs in use. There are various factors at play here: Can you load data fast enough to feed all of the GPUs attached to your system(s)? Is your model's runtime large enough to hide the latencies of transferring the weights from controllers to devices? What are your devices doing while your controller applies the gradient updates?\n",
    "* When you handled all of the above, you might still not see improvements in training progress (as measured in changes of the loss). Sure, you will (hopefully) burn through your data at X times the speed of a single device (personal experience: ~2.7 times speedup with 3 GPUs in synchronous training is possible), but does that translate to faster training? Effectively, synchronous training increases the minibatch size, meaning that you can expect to get more accurate estimates of the true gradient. Translating this into a faster decrease of the loss is still up to you (e.g., by increasing the learning rate when using standard SGD). This latter point is a bit different with asynchronous training, since that really *does* cause more parameter updates to occur; though I have not yet seen a systematic study of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Training with Multiple GPUs\n",
    "In this section, we will be using in-graph replication with synchronous training to parallelize the model's training across multiple GPUs installed in the local machine. The controller device will be the CPU, meaning that all variables will live on this device and will be copied to the GPUs in each step.\n",
    "The model that we will be using here is an MNIST classifier. Note that this model is much too small to greatly benefit from multi GPU computation -- most of the time will be spend on transferring weights and training data between CPU and GPU.\n",
    "\n",
    "For the way that the model is setup, see the original tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def core_model(input_img, num_classes=10):\n",
    "    \"\"\"\n",
    "      A simple model to perform classification on 28x28 grayscale images in MNIST style.\n",
    "\n",
    "      Args:\n",
    "        input_img:  A floating point tensor with a shape that is reshapable to batchsizex28x28. It\n",
    "            represents the inputs to the model\n",
    "        num_classes:  The number of classes\n",
    "    \"\"\"\n",
    "    net = tf.reshape(input_img, [-1, 28, 28, 1])\n",
    "    net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=[5, 5],\n",
    "                           padding=\"same\", activation=tf.nn.relu,\n",
    "                           name=\"conv2d_1\")\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=[5, 5],\n",
    "                           padding=\"same\", activation=tf.nn.relu,\n",
    "                           name=\"conv2d_2\")\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=[2, 2], strides=2)\n",
    "    net = tf.reshape(net, [-1, 7 * 7 * 64])\n",
    "    net = tf.layers.dense(inputs=net, units=1024, name=\"dense_1\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=net, units=num_classes, name=\"dense_2\")\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will define a training model that turns our core model into a loss that can be passed to an optimizer. To keep things a bit shorter, we will simply return the loss and nothing else.\n",
    "Note one crucial difference here: Instead of a tensor representing the inputs to the model, this function takes a *function that produces such a tensor*. This will be crucial for parallelizing the model later on, since each copy of the model should have its own such tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_model(input_fn):\n",
    "    inputs = input_fn()\n",
    "    image = inputs[0]\n",
    "    label = tf.cast(inputs[1], tf.int32)\n",
    "    logits = core_model(image)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be taking a slightly unorthodox (and inefficient) short-cut to get the MNIST data shipping with Tensorflow in a dataset-API format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_dataset(epochs=5, batch_size=128):\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist_data = input_data.read_data_sets(\"data\")\n",
    "    all_data_points = mnist_data.train.next_batch(60000)\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices(all_data_points)\n",
    "    dataset = dataset.repeat(epochs).shuffle(10000).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train this model as usual before we go to the parallel setting (all non-essential steps like saving, summaries, etc. are skipped):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_training(update_op, loss):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        try:\n",
    "            step = 0\n",
    "            while True:\n",
    "                _, loss_value = sess.run((update_op, loss))\n",
    "                if step % 100 == 0:\n",
    "                    print('Step {} with loss {}'.format(step, loss_value))\n",
    "                step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            # we're through the dataset\n",
    "            pass\n",
    "    print('Final loss: {}'.format(loss_value))\n",
    "\n",
    "def serial_training(model_fn, dataset):\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    loss = model_fn(lambda: iterator.get_next())\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1E-3)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    update_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    do_training(update_op, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0 with loss 2.30421590805\n",
      "Step 100 with loss 0.0781030803919\n",
      "Step 200 with loss 0.0625318139791\n",
      "Step 300 with loss 0.0476590171456\n",
      "Step 400 with loss 0.0532032027841\n",
      "Step 500 with loss 0.100487187505\n",
      "Step 600 with loss 0.011965803802\n",
      "Step 700 with loss 0.0720254331827\n",
      "Step 800 with loss 0.017533345148\n",
      "Step 900 with loss 0.00320947216824\n",
      "Final loss: 0.187678039074\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "serial_training(training_model, training_dataset(epochs=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll skip evaluation, since you probably have a good idea of how to do that yourself. Let's get to the interesting part: Replication across multiple GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multi-GPU version\n",
    "\n",
    "There are two different perspectives one can take on this: One could either attempt to create a function that takes a model function as an input and returns a model function for the parallelized model (philosophically, this means that parallelity is a property of the model), or one can create a specialized training procedure that takes a model function and trains the modle in parallel (parallelity is a property of the optimization process). *I am heavily leaning towards the latter*, since parallelization of a model requires some knowledge of the optimizer in use.\n",
    "\n",
    "Here is a short road-map of what we will be doing:\n",
    "* We will create a specialized training function that trains a model given by its model function on multiple devices,\n",
    "* this function will create one copy of the model (called a *tower*) per device and instruct it to compute forward and backward passes,\n",
    "* the gradients will then be averaged and applied on the controller device where all the model's variables reside.\n",
    "\n",
    "This already alludes to why parallelity should be seen as a property of the training procedure, not the model: We will need to split the minimize step of the optimizer into `compute_gradients` (performed on the devices) and `apply_gradients` (running on the controller). Thus whatever makes the parallel training work will need to know about the optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallel_training(model_fn, dataset):\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    def input_fn():\n",
    "        with tf.device(None):\n",
    "            # remove any device specifications for the input data\n",
    "            return iterator.get_next()\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1E-3)\n",
    "    update_op, loss = create_parallel_optimization(model_fn,\n",
    "                                                   input_fn,\n",
    "                                                   optimizer)\n",
    "\n",
    "    do_training(update_op, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final training function: All the magic happens in `create_parallel_optimization`, and we will slowly work towards this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the Device for an Operation\n",
    "TensorFlow allows the specification of a device for each of the operations you create. This works by using `tf.device`, as in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def device_example():\n",
    "    # allocate variables on the CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        M = tf.get_variable('M', shape=[10,8], dtype=tf.float32)\n",
    "        x = tf.get_variable('x', shape=[8, 1], dtype=tf.float32)\n",
    "    # perform the operation on the fi=rst GPU device\n",
    "    with tf.device('/gpu:0'):\n",
    "        y = tf.matmul(M, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all variables in our setting should live on the controller device, but all operations should run on the operating devices, we can either explicitly allocate all variables beforehand (tedious!) or make use of the fact that `tf.device` can also take a function as its argument: This function then dynamically decides which operation to put where.\n",
    "\n",
    "The following function allows us to construct a suitable argument for `tf.device` that places all variables on the controlling device, and everything else on a device of our choice (all credit goes to the author with that GitHub issue):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PS_OPS = [\n",
    "    'Variable', 'VariableV2', 'AutoReloadVariable', 'MutableHashTable',\n",
    "    'MutableHashTableOfTensors', 'MutableDenseHashTable'\n",
    "]\n",
    "\n",
    "# see https://github.com/tensorflow/tensorflow/issues/9517\n",
    "def assign_to_device(device, ps_device):\n",
    "    \"\"\"Returns a function to place variables on the ps_device.\n",
    "\n",
    "    Args:\n",
    "        device: Device for everything but variables\n",
    "        ps_device: Device to put the variables on. Example values are /GPU:0 and /CPU:0.\n",
    "\n",
    "    If ps_device is not set then the variables will be placed on the device.\n",
    "    The best device for shared varibles depends on the platform as well as the\n",
    "    model. Start with CPU:0 and then test GPU:0 to see if there is an\n",
    "    improvement.\n",
    "    \"\"\"\n",
    "    def _assign(op):\n",
    "        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n",
    "        if node_def.op in PS_OPS:\n",
    "            return ps_device\n",
    "        else:\n",
    "            return device\n",
    "    return _assign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same example as above to demonstrate its usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def device_example_2():\n",
    "    # allocate variables on the CPU, perform the operation on the first GPU device\n",
    "    with tf.device(assign_to_device('/gpu:0', '/cpu:0')):\n",
    "        M = tf.get_variable('M', shape=[10,8], dtype=tf.float32)\n",
    "        x = tf.get_variable('x', shape=[8, 1], dtype=tf.float32)\n",
    "        y = tf.matmul(M, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside: You may find it helpful to allow TensorFlow to violate your device specifications when necessary (*soft placement*) or to check which devices are used for which operations. Both of it is possible with customizing the session which is running the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def device_options():\n",
    "    config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TensorBoard's graph view, you can also inspect which operations are placed on which device, but that view only shows static information (and not TensorFlow's placement decisions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reusing Variables\n",
    "One crucial point when parallelizing a model is that you need to ensure that multiple copies of the same operation share common weights. For this, it is necessary to briefly talk about *variable scopes*: Variables scopes are like name scopes, except that they affect variables instead of operations. Two variables can have the same name as long as they live in different scopes. The `tf.get_variable` function is essentially performing variable look-up in the local scope, and if a variable doesn't exist yet, it is created for you.\n",
    "A vital feature is that you can *reuse* variable scopes, that is, you can go back into a variable scope you already created earlier, and access variables you used before. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_scope_example():\n",
    "    with tf.variable_scope('test_scope'):\n",
    "        M = tf.get_variable('matrix1', shape=[10, 8], dtype=tf.float32)\n",
    "        x = tf.get_variable('matrix2', shape=[8, 1], dtype=tf.float32)\n",
    "        y = tf.matmul(M, x)\n",
    "    # Here, we are instructing TensorFlow to reuse to variables declared above.\n",
    "    # The `M` from above and the `N` from below reference the same Tensor!\n",
    "    with tf.variable_scope('test_scope', reuse=True):\n",
    "        N = tf.get_variable('matrix1', shape=[10, 8], dtype=tf.float32)\n",
    "        z = tf.get_variable('matrix2', shape=[8, 1], dtype=tf.float32)\n",
    "        w = tf.matmul(N, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things worth noting:\n",
    "* By setting `reuse=True` above, we state that *all* variables we request have already been defined. We cannot declare new ones in that scope.\n",
    "* Obviously, we have to ensure that the variables agree on their parameters. For example, in the code above we could not change `N` to have shape `[10, 10]` in the second scope because it already exists with a shape of `[10, 8]`.\n",
    "* In many places of higher-level APIs, variable scopes are created automatically. They often use consecutive naming when no default name is given. Variable-reusing only works when the variable scopes have the exact same name and path, so **always name your layers when using TFLayers or Slim**. For example, the following will **not** reuse variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_scope_layers_failure():\n",
    "    input_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 10, 10, 1])\n",
    "    with tf.variable_scope('test_scope'):\n",
    "        tf.layers.conv2d(input_tensor, filters=10, kernel_size=[5, 5])\n",
    "\n",
    "    # This will NOT reuse variables, since both `conv2d` implicitly create a variable scope with a\n",
    "    # fresh name. Add `name='layer_name'` to make it work.\n",
    "    with tf.variable_scope('test_scope', reuse=True):\n",
    "        tf.layers.conv2d(input_tensor, filters=10, kernel_size=[5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_scope_layers_correct():\n",
    "    input_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 10, 10, 1])\n",
    "    with tf.variable_scope('test_scope'):\n",
    "        tf.layers.conv2d(input_tensor, filters=10, kernel_size=[5, 5],\n",
    "                         name='my_conv')\n",
    "    # This does what it is supposed to do since both convolutional layers have been given\n",
    "    # the same name\n",
    "    with tf.variable_scope('test_scope', reuse=True):\n",
    "        tf.layers.conv2d(input_tensor, filters=10, kernel_size=[5, 5],\n",
    "                         name='my_conv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of setting `reuse=True`, we can also call `reuse_variables` on the scope to achieve the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_scope_example_2():\n",
    "    with tf.variable_scope('test_scope') as vscope:\n",
    "        M = tf.get_variable('matrix1', shape=[10, 8], dtype=tf.float32)\n",
    "        x = tf.get_variable('matrix2', shape=[8, 1], dtype=tf.float32)\n",
    "        y = tf.matmul(M, x)\n",
    "        \n",
    "        vscope.reuse_variables()\n",
    "        \n",
    "        # variables are reused here\n",
    "        N = tf.get_variable('matrix1', shape=[10, 8], dtype=tf.float32)\n",
    "        z = tf.get_variable('matrix2', shape=[8, 1], dtype=tf.float32)\n",
    "        w = tf.matmul(N, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Parallel Optimization Routine\n",
    "Now we are ready to assemble the parts. There are still some unknowns in the following code; we will discuss them shortly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_parallel_optimization(model_fn, input_fn, optimizer, controller=\"/cpu:0\"):\n",
    "    # This function is defined below; it returns a list of device ids like\n",
    "    # `['/gpu:0', '/gpu:1']`\n",
    "    devices = get_available_gpus()\n",
    "        \n",
    "    # This list keeps track of the gradients per tower and the losses\n",
    "    tower_grads = []\n",
    "    losses = []\n",
    "    \n",
    "    # Get the current variable scope so we can reuse all variables we need once we get\n",
    "    # to the second iteration of the loop below\n",
    "    with tf.variable_scope(tf.get_variable_scope()) as outer_scope:\n",
    "        for i, id in enumerate(devices):\n",
    "            name = 'tower_{}'.format(i)\n",
    "            # Use the assign_to_device function to ensure that variables are created on the\n",
    "            # controller.\n",
    "            with tf.device(assign_to_device(id, controller)), tf.name_scope(name):\n",
    "                \n",
    "                # Compute loss and gradients, but don't apply them yet\n",
    "                loss = model_fn(input_fn)\n",
    "                \n",
    "                with tf.name_scope(\"compute_gradients\"):\n",
    "                    # `compute_gradients` returns a list of (gradient, variable) pairs\n",
    "                    grads = optimizer.compute_gradients(loss)\n",
    "                    tower_grads.append(grads)\n",
    "                    \n",
    "                losses.append(loss)\n",
    "            \n",
    "            # After the first iteration, we want to reuse the variables.\n",
    "            outer_scope.reuse_variables()\n",
    "                \n",
    "    # Apply the gradients on the controlling device\n",
    "    with tf.name_scope(\"apply_gradients\"), tf.device(controller):\n",
    "        # Note that what we are doing here mathematically is equivalent to returning the\n",
    "        # average loss over the towers and compute the gradients relative to that.\n",
    "        # Unfortunately, this would place all gradient-computations on one device, which is\n",
    "        # why we had to compute the gradients above per tower and need to average them here.\n",
    "        \n",
    "        # This function is defined below; it takes the list of (gradient, variable) lists\n",
    "        # and turns it into a single (gradient, variables) list.\n",
    "        gradients = average_gradients(tower_grads)\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        apply_gradient_op = optimizer.apply_gradients(gradients, global_step)\n",
    "        avg_loss = tf.reduce_mean(losses)\n",
    "\n",
    "    return apply_gradient_op, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't so difficult, was it?\n",
    "\n",
    "At this point, I should point out that the `input_fn` has been called once for each model. This is critical: If we had called `iterator.get_next()` once and passed the resulting tensor to each of the models, all of the models would use the same data in every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_available_gpus` function isn't worth further discussion (credits to the original author at the given source):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "        Returns a list of the identifiers of all visible GPUs.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only missing part is the `average_gradients` function. It does what you'd think it would do (credits to the original author at the given source, some modifications have been made):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see\n",
    "# https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L101\n",
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list ranges\n",
    "        over the devices. The inner list ranges over the different variables.\n",
    "    Returns:\n",
    "         List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "         across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = [g for g, _ in grad_and_vars]\n",
    "        grad = tf.reduce_mean(grads, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0 with loss 2.29874181747\n",
      "Step 100 with loss 0.114577434957\n",
      "Step 200 with loss 0.0739449113607\n",
      "Step 300 with loss 0.0614554695785\n",
      "Step 400 with loss 0.013178229332\n",
      "Final loss: 0.0271248538047\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "parallel_training(training_model, training_dataset(epochs=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the number of steps is about half as much as that observed with one GPU. This makes perfect sense, since we have effectively doubled the minibatch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective Training on Multiple GPUs\n",
    "Here are a few general pointers for making sure that you really benefit from multi GPU systems:\n",
    "\n",
    "* Make sure that you have enough data available to feed your GPUs. This may mean that you need use multi-threading in your input pipeline. If you are using the dataset API, this is as simple as using `map` with the right parameters. Experimenting with the number of threads to use can be helpful, since there is a tradeoff between using the CPU to load data and using the CPU to average gradients (assuming your CPU is the controller device).\n",
    "* Depending on your setup, it may be advantageous to use one of your GPUs as a controller device. This would lead to much faster averaging of the gradients (and for a large model, this step is non-negligible), but it is only helpful when your GPUs have direct access to each other's memory - otherwise you are just adding more and more inter-device transfers and slow the system down.\n",
    "* If your controller device is the CPU, you should think about what your GPUs will be doing while the CPU is averaging their gradients and applying them. They cannot start working on the next batch of inputs, since they are still waiting for the new weights. However, if your model is using data augmentation, your GPUs could already work on augmenting the next batch of images. This intra-model pipelining can be implemented using a [StagingArea](https://www.tensorflow.org/api_docs/python/tf/contrib/staging/StagingArea) and probably warrants a small tutorial on its own. Note that `tf.contrib.staging.StagingArea` lives in the `contrib` namespace and is therefore subject to changes. It will likely be superseded in the near future by staging support built-in into the dataset API ([see here](https://docs.google.com/presentation/d/16kHNtQslt-yuJ3w8GIx-eEH6t_AvFeQOchqGRFpAD7U/edit#slide=id.g254d08e080_0_322))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model using Distributed TensorFlow\n",
    "Distributed TensorFlow allows training of models across multiple computers, but it can also be used on a single machine (which is what we will do here, for testing purposes). The basic principles remain the same.\n",
    "\n",
    "Since distributed TensorFlow requires multiple TensorFlow instances to run at once, it is difficult to present everything in a single notebook. Therefore, we will go over the necessary code, but not actually execute it here.\n",
    "\n",
    "As already pointed out before, there are two different kinds of entities that make up a distributed TensorFlow training procedure: *Parameter servers* (PS) and *workers*. They need different code to run, but luckily the code for the parameter servers is very simple. Before a distributed training is started, the workers and parameter servers that should participate need to be specified. This is called a *cluster specification*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_specification = {\n",
    "    \"ps\": [\"localhost:2222\"], # list of parameter servers,\n",
    "    \"worker\": [\"localhost:2223\", \"localhost:2224\"] # list of workers\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terminology here is that our cluster specification has to *jobs* (`ps` and `worker`), and for each of them we specify *tasks* by giving the address of the TensorFlow instance that will work on that task.\n",
    "\n",
    "In our case, these are all local task. This means we will have 3 TensorFlow instances running (my local machine has 2 GPUs for two worker tasks, plus the parameter server). By default, each TensorFlow instance will try to claim *all memory on all visible GPUs*! To prevent this, set the `CUDA_VISIBLE_DEVICES` environment variable before starting each task. For example, `CUDA_VISIBLE_DEVICES=0` to only set the first GPU as visible.\n",
    "\n",
    "That said, here is the code that needs to be run for parameter server tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_parameter_server(task_index, cluster_specifcation):\n",
    "    cluster_spec = tf.train.ClusterSpec(cluster_specification)\n",
    "    server = tf.train.Server(cluster_spec, job_name='ps', task_index=task_index)\n",
    "    server.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't even depend on (or know about) the model you are running.\n",
    "\n",
    "For the worker jobs, we will need slightly more code, but not all that much. We will use `tf.train.MonitoredTrainingSession` instead of `tf.Session` below, since that already includes a bunch of useful helpers that make distributed training easier. See below for a short discussion about `tf.train.MonitoredTrainingSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_worker(task_index, cluster_specification, dataset):  \n",
    "    cluster_spec = tf.train.ClusterSpec(cluster_specification)\n",
    "    server = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=task_index)\n",
    "    \n",
    "    worker_device = \"/job:worker/task:{}\".format(task_index)\n",
    "    # `tf.train.replace_device_setter` automatically determines where to place variables\n",
    "    with tf.device(tf.train.replica_device_setter(worker_device=worker_device,\n",
    "                                                  cluster=cluster_spec)):\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        loss = training_model(lambda: iterator.get_next())\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1E-3)\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        update_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        \n",
    "        # `tf.train.MonitoredTrainingSession` can be used as a drop-in replacement\n",
    "        # for regular sessions.\n",
    "        with tf.train.MonitoredTrainingSession(master=server.target,\n",
    "                                               is_chief=task_index == 0) as sess:\n",
    "            while not sess.should_stop():\n",
    "                sess.run(update_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few details we should quickly talk about.\n",
    "\n",
    "**Setting devices** - \n",
    "As before, each variable needs to be set to a device. Here, we are using `tf.train.replica_device_setter` to do that automatically. It could also be done manually by using e.g. `tf.device('/job:ps/task:0)` to specify that something should be performed by the first parameter server. The `tf.train.replica_devise_setter` is a bit more conservative than our solution above, since (by default) it only allocates `tf.Variable` on parameter servers, but (as with many other aspects of its behaviour) this can be customized. Similarly, the procedure by default uses a round-robin style assignment of variables, but you can also define custom parameter distribution strategies.\n",
    "\n",
    "**The Monitored Training Session** - \n",
    "In the example above, we have used `tf.train.MonitoredTrainingSession` instead of `tf.Session`. This is a wrapper around sessions that handles\n",
    " * saving summaries,\n",
    " * saving checkpoints and restoring from them after crashes,\n",
    " * measuring steps/second,\n",
    " * variable initialization,\n",
    " * distributed training.\n",
    "Most of its behaviour can be changed via its arguments (they are mostly pretty self-explanatory), but some features require you to add custom *hooks* to the session. A common use case for that would be when you have two classes of summaries: Ones that you want to evaluate frequently, and other more expensive summary operations that should only be evaluated every once in a while. This will need a custom hook. Similarly, starting training from some given initialization needs some additional work with this session-like object.\n",
    "For more information, see [the TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession). Take special note of the `scaffold` parameter; it looks innocent but once you try to do anything non-trivial with `MonitoredTrainingSession`, you will need it. See [its documentation](https://www.tensorflow.org/api_docs/python/tf/train/Scaffold).\n",
    "\n",
    "**Chief and Master** - \n",
    "These, `master` and `is_chief`, are two of the arguments of `MonitoredTrainingSession` that need some explanation. First, *chief* is short for *chief worker*. This is a designated worker job with special responsibilities, like initializing and recovering the model after failures. For most intents and purposes, you should not worry about it. Just make sure there is a chief (in our case, the worker with task index 0).\n",
    "The *master* on the other hand is responsible for actually coordinating the work between different jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Training Process\n",
    "We are almost there, but still need to start the jobs. Execute the following in three separate terminals to see everything in action (remember to make sure to set `CUDA_VISIBLE_DEVICES` appropriately beforehand):\n",
    "\n",
    "`start_parameter_server(0, cluster_specification)`\n",
    "\n",
    "`start_worker(0, cluster_specification, training_dataset())`\n",
    "\n",
    "`start_worker(1, cluster_specification, training_dataset())`\n",
    "\n",
    "Your jobs should now slowly start to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All in all, distributed training is probably easier to set up than manual in-graph replication, but this approach will not necessarily yield the same performance: It requires interprocess or network communication, which can be quite a slow down.\n",
    "On my local machine, I get the following performance:\n",
    "* Single GPU: 200 batches of 128 images per second\n",
    "* Two GPUs, in-graph replication: 80 batches of 2x128 images per second\n",
    "* Two GPUs, distributed (2 workers): 50 batches of 2x128 images per second\n",
    "\n",
    "This is *even though* the distributed training works asynchronously, whereas the in-graph replication example uses synchronous training. Remember that this is not a real-world example and, as already said, the model is *much* too small to expect any performance gains from parallelization, since most of the time will be spent on communication overhead, transfers, etc.\n",
    "\n",
    "For larger models, these numbers will change, but you should not expect distributed TensorFlow to beat in-graph replication (assuming both are runnig on a single machine). When you have many machines available, distributed TensorFlow is the way to go, though you may want to consider doing synchronized multi GPU training on each machine separately and aggregate across machines by using distributed TensorFlow. At some point, it would also be wise to add more parameter servers to the cluster to reduce the load on that machine and its network connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "* For synchronous distributed training, take a look at [SyncReplicasOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer),\n",
    "* TensorFlow's documentation has a [page on distributed training](https://www.tensorflow.org/deploy/distributed). There is not much on it that we haven't covered here as well, but the talk linked on that page is worth watching.\n",
    "* The in-graph replication code is inspired by [this implementation](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py), though this is already somewhat older.\n",
    "* There are frameworks that offer alternatives to the TensorFlow built-in support for distributed training. Some like Uber's [Horovod](https://eng.uber.com/horovod/) claim to give much better performance, but I have not tested them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
